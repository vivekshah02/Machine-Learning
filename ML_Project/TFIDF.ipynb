{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47b59bd-3ccd-45a1-9e9b-b51c13432439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature extraction completed and saved to tfidf_features_add_data_cleaned.csv!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load dataset\n",
    "# Assuming your Excel file has columns: 'Question', 'Answer', 'Strength'\n",
    "data = pd.read_excel('Audio_Team_FinalCleaned.xlsx')\n",
    "\n",
    "# Combine Questions and Answers into a single text column\n",
    "data['text'] = data['Question'] + \" \" + data['Answer']\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit and transform the text data to create TF-IDF embeddings\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Convert the sparse matrix to a dense format (optional)\n",
    "tfidf_embeddings_dense = tfidf_embeddings.todense()\n",
    "\n",
    "# Convert the dense TF-IDF matrix to a DataFrame for easier handling\n",
    "tfidf_df = pd.DataFrame(tfidf_embeddings_dense, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Create a new DataFrame to hold the features\n",
    "features = []\n",
    "\n",
    "# Iterate through the original dataset to append strength and embeddings\n",
    "for index, row in data.iterrows():\n",
    "    # Get the strength value\n",
    "    strength = row['Strength']\n",
    "    \n",
    "    # Get the corresponding TF-IDF embedding for the current row\n",
    "    embedding = tfidf_df.iloc[index].values  # Extract the embedding for this row\n",
    "    \n",
    "    # Append the strength and embedding as a dictionary\n",
    "    features.append({\n",
    "        'strength': strength,\n",
    "        'embedding': embedding  # TF-IDF embedding as a feature\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "features_df = pd.DataFrame(features)\n",
    "\n",
    "# Convert the 'embedding' column (which contains arrays) into a format that can be saved in a CSV\n",
    "# This is done by converting the array into a string representation\n",
    "features_df['embedding'] = features_df['embedding'].apply(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "# Save the DataFrame with strength and embeddings to a new CSV\n",
    "features_df.to_csv('tfidf_features_add_data_cleaned.csv', index=False)\n",
    "\n",
    "print(\"TF-IDF feature extraction completed and saved to tfidf_features_add_data_cleaned.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91831ef3-f431-497b-9f94-637fea4b782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training class distribution: Counter({2: 164, 1: 132, 0: 110})\n",
      "Resampled training class distribution: Counter({2: 164, 1: 164, 0: 164})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:22:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+-------------+----------+------------+\n",
      "| Classifier          |   Accuracy |   Precision |   Recall |   F1-Score |\n",
      "+=====================+============+=============+==========+============+\n",
      "| Random Forest       |   0.54902  |    0.526465 | 0.54902  |   0.52428  |\n",
      "+---------------------+------------+-------------+----------+------------+\n",
      "| SVM                 |   0.578431 |    0.611515 | 0.578431 |   0.559992 |\n",
      "+---------------------+------------+-------------+----------+------------+\n",
      "| Decision Tree       |   0.352941 |    0.363627 | 0.352941 |   0.355987 |\n",
      "+---------------------+------------+-------------+----------+------------+\n",
      "| AdaBoost            |   0.509804 |    0.553977 | 0.509804 |   0.514429 |\n",
      "+---------------------+------------+-------------+----------+------------+\n",
      "| XGBoost             |   0.441176 |    0.436439 | 0.441176 |   0.43805  |\n",
      "+---------------------+------------+-------------+----------+------------+\n",
      "| CatBoost            |   0.480392 |    0.478227 | 0.480392 |   0.478511 |\n",
      "+---------------------+------------+-------------+----------+------------+\n",
      "| Naive Bayes         |   0.519608 |    0.560306 | 0.519608 |   0.528497 |\n",
      "+---------------------+------------+-------------+----------+------------+\n",
      "| Logistic Regression |   0.529412 |    0.540168 | 0.529412 |   0.531803 |\n",
      "+---------------------+------------+-------------+----------+------------+\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.61      0.66        33\n",
      "           0       0.43      0.41      0.42        32\n",
      "           1       0.48      0.57      0.52        37\n",
      "\n",
      "    accuracy                           0.53       102\n",
      "   macro avg       0.54      0.53      0.53       102\n",
      "weighted avg       0.54      0.53      0.53       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel('Audio_Team_FinalCleaned.xlsx')\n",
    "\n",
    "# Combine Questions and Answers into a single text column\n",
    "data['text'] = data['Question'] + \" \" + data['Answer']\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit and transform the text data to create TF-IDF embeddings\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(data['text']).toarray()\n",
    "\n",
    "# Create a DataFrame for the TF-IDF embeddings\n",
    "features_df = pd.DataFrame(tfidf_embeddings)\n",
    "\n",
    "# Add strength labels to the DataFrame\n",
    "features_df['strength'] = data['Strength']\n",
    "\n",
    "# Map strength labels from {-1, 0, 1} to {0, 1, 2}\n",
    "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "features_df['mapped_strength'] = features_df['strength'].map(label_mapping)\n",
    "\n",
    "# Split the features and labels\n",
    "X = features_df.drop(columns=['strength', 'mapped_strength']).values  # Use only the TF-IDF features\n",
    "y = features_df['mapped_strength'].values  # Use the mapped strength column\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Verify class distribution after SMOTE\n",
    "print(\"Original training class distribution:\", Counter(y_train))\n",
    "print(\"Resampled training class distribution:\", Counter(y_resampled))\n",
    "\n",
    "# List of classifiers with optimized hyperparameters for Random Forest\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(max_depth=10, min_samples_split=2, n_estimators=50, random_state=42),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'XGBoost': XGBClassifier(eval_metric='mlogloss', use_label_encoder=False),\n",
    "    'CatBoost': CatBoostClassifier(silent=True),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000)  # Linear regression model\n",
    "}\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over classifiers and store performance metrics\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_resampled, y_resampled)  # Train on resampled (balanced) data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Append the results\n",
    "    results.append([name, accuracy, precision, recall, f1])\n",
    "\n",
    "# Create a table to show results\n",
    "headers = [\"Classifier\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "print(tabulate(results, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "# If needed: Map predictions back to original labels\n",
    "inverse_mapping = {0: -1, 1: 0, 2: 1}\n",
    "y_test_original = np.vectorize(inverse_mapping.get)(y_test)\n",
    "y_pred_original = np.vectorize(inverse_mapping.get)(y_pred)\n",
    "\n",
    "# Print classification report for original labels\n",
    "print(classification_report(y_test_original, y_pred_original))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab462ac-f0ff-4052-977a-5980fee7208d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
